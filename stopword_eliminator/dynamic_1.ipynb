{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-20T15:31:49.252506Z",
     "start_time": "2024-11-20T15:31:42.711800Z"
    }
   },
   "source": [
    "from tokenizer.ml_based_tokenizer import main2\n",
    "from tokenizer.rule_based_tokenizer import InputType\n",
    "import pickle"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def main(input, input_type):\n",
    "    # If the input is a file path or a string\n",
    "    if input_type != InputType.LIST:\n",
    "        # First, tokenize the text in the file/string using the ml based tokenizer.\n",
    "        if input_type == InputType.FILE_PATH:\n",
    "            input_tokens = main2(input, InputType.FILE_PATH)\n",
    "        else:\n",
    "            input_tokens = main2(input, InputType.STRING)\n",
    "\n",
    "    # If input is directly provided as a list of tokens, just use it\n",
    "    else:\n",
    "        input_tokens = input\n",
    "\n",
    "    # Remove tokens with non-alphabetical characters and feed the resulting token list to the stemmer\n",
    "    tokens = []\n",
    "    for token in input_tokens:\n",
    "        if token.isalpha():\n",
    "            tokens.append(token)"
   ],
   "id": "e5ad0c8a6a01b289"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7a846f2e3feb1e52"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
